# Goal

Train a **tiny code-focused GPT-style model (~125 M params)** on your GitHub PR / comment corpus using your AMD GPU.

---

## 1Ô∏è‚É£  Environment setup (ROCm / PyTorch)

> Tested on Fedora, Ubuntu 22.04+, Arch with kernel ‚â• 6.8.

```bash
# ROCm and PyTorch (use ROCm wheels, not CUDA)
sudo dnf install rocm-dev rocm-libs hipblas rocblas miopen-hip

python3 -m venv llm
source llm/bin/activate

pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0
pip install transformers datasets accelerate sentencepiece tiktoken bitsandbytes
```

Check GPU:

```python
python - <<'PY'
import torch; print(torch.cuda.is_available(), torch.cuda.get_device_name(0))
PY
```

Expect `True`, and something like `AMD Radeon RX 6700 XT`.

---

## 2Ô∏è‚É£  Tokenizer

Use the corpus produced by your Go pipeline (plain-text or JSONL).
Train SentencePiece ‚Üí 50 K vocab:

```bash
spm_train \
  --input=corpus.txt \
  --model_prefix=golang_llm \
  --vocab_size=50000 \
  --character_coverage=1.0 \
  --model_type=bpe
```

You‚Äôll get:

```text
golang_llm.model
golang_llm.vocab
```

---

## 3Ô∏è‚É£  Dataset prep

Convert your serialized JSONL into plain text (prompt‚Äìresponse pairs or code‚Äìcomment lines).
Then tokenize:

```python
from datasets import load_dataset
from transformers import AutoTokenizer

tok = AutoTokenizer.from_pretrained(".", model_max_length=1024, padding_side="right", truncation_side="right")
tok.add_special_tokens({"pad_token": "<|pad|>"})

ds = load_dataset("text", data_files="corpus.txt")
def tok_fn(batch): return tok(batch["text"], truncation=True, padding="max_length")
tok_ds = ds.map(tok_fn, batched=True, remove_columns=["text"])
tok_ds.save_to_disk("tok_dataset")
```

---

## 4Ô∏è‚É£  Model config (125 M params)

```python
from transformers import GPT2Config, GPT2LMHeadModel

cfg = GPT2Config(
    vocab_size=50000,
    n_positions=1024,
    n_embd=768,
    n_layer=12,
    n_head=12,
)
model = GPT2LMHeadModel(cfg)
model.save_pretrained("golang_gpt2_125m")
```

---

## 5Ô∏è‚É£  Training script

Create `train.py`:

```python
from transformers import Trainer, TrainingArguments, GPT2LMHeadModel, AutoTokenizer, DataCollatorForLanguageModeling
from datasets import load_from_disk

model = GPT2LMHeadModel.from_pretrained("golang_gpt2_125m")
tok = AutoTokenizer.from_pretrained(".", model_max_length=1024)
tok.pad_token = "<|pad|>"

data = load_from_disk("tok_dataset")

args = TrainingArguments(
    output_dir="checkpoints",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    learning_rate=5e-4,
    fp16=True,                       # ROCm supports this
    save_total_limit=2,
    logging_steps=50,
    save_steps=1000,
    report_to="none",
)

collator = DataCollatorForLanguageModeling(tok, mlm=False)
trainer = Trainer(model=model, args=args, train_dataset=data["train"], data_collator=collator)
trainer.train()
```

Run it:

```bash
python train.py
```

> With batch = 2 √ó grad accum 8 and seq = 1024, fits in ‚âà 10-11 GB VRAM.
> Expect **~5 tokens/sec ‚Üí 5‚Äì7 days** for ~10 B tokens (scale down to 1 B tokens for ‚âà 12 h run).

---

## 6Ô∏è‚É£  Evaluation / sampling

```python
from transformers import pipeline
gen = pipeline("text-generation", model="checkpoints", tokenizer=".")
out = gen("Write a Go function that reads a JSON file:", max_length=200)
print(out[0]["generated_text"])
```

---

## 7Ô∏è‚É£  Export to GGUF / Ollama (optional)

Once you‚Äôre happy with results:

```bash
python convert.py --from transformers --to gguf --model checkpoints
ollama create golang-llm -f Modelfile
```

Then you can use:

```bash
ollama run golang-llm
```

and call it from LangChain or your Go service.

---

## 8Ô∏è‚É£  What you‚Äôll get

* A working tokenizer trained on your GitHub corpus
* A small 125 M GPT that understands Go code structure + PR language
* Full pipeline test (data ‚Üí tokens ‚Üí model ‚Üí inference)

---

## üß†  Optional optimizations

* Use `bitsandbytes` 8-bit optimizer: halves VRAM use.
* Use `torch.compile()` on PyTorch 2.1 + ROCm 6 for ‚âà 10 % speedup.
* Increase `gradient_accumulation_steps` if you run out of VRAM.

---

## ‚úÖ  Summary

| Stage              | Tool                       | Runtime          |
| ------------------ | -------------------------- | ---------------- |
| Data ‚Üí text        | Go pipeline                | CPU              |
| Tokenizer          | SentencePiece              | CPU              |
| Model train        | PyTorch (ROCm)             | GPU (RX 6700 XT) |
| Export ‚Üí inference | convert ‚Üí Ollama / LocalAI | CPU / GPU        |

Expected runtime (10 B tokens): **‚âà 5‚Äì7 days**.
You can shorten dramatically by training on fewer tokens first to verify.

---
